{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2\n",
    "#https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "#https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9\n",
    "# https://learning.oreilly.com/videos/reinforcement-learning-and/9781491995006/9781491995006-video312885\n",
    "import gym\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning using Frozen Lake\n",
    "OpenGym provides enviorments to test reinforcement learning algorithms on. We choose to use the Fozen Lake environment. It's essentially a grid representing a frozen lake with holes in it and the goal is to cross the lake without falling into one of these holes. \n",
    "\n",
    "#### Paramters\n",
    "alpha is the learning rate\n",
    "gamma is the discount factor. the amount of importance we give for future rewards when gamma is close to zero, agent favors immediate rewards. We want a HIGH gamma because the goal is to survive the whole lake.\n",
    "\n",
    "#### Equation\n",
    "Q[s, a] = Q[s, a] + alpha*(R + gamma*Max[Q(s’, A)] — Q[s, a])\n",
    "\n",
    "So at state 3 our equation might look like: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 6000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "print (\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results\n",
    "\n",
    "### Changing number of episodes\n",
    ".9 gamma .628 alpha\n",
    "50: .0\n",
    "500: .0\n",
    "1000:.001\n",
    "5000:.214\n",
    "50000: .2775\n",
    "\n",
    "### Changing Gamma\n",
    ".628 alpha 5000 episodes\n",
    ".9: .0528\n",
    ".95: .06\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
