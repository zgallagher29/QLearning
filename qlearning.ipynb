{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2\n",
    "#https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "#https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9\n",
    "# https://learning.oreilly.com/videos/reinforcement-learning-and/9781491995006/9781491995006-video312885\n",
    "import gym\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning using Frozen Lake\n",
    "OpenGym provides enviorments to test reinforcement learning algorithms on. We choose to use the Fozen Lake environment. It's essentially a grid representing a frozen lake with holes in it and the goal is to cross the lake without falling into one of these holes. \n",
    "\n",
    "#### Paramters\n",
    "alpha is the learning rate\n",
    "gamma is the discount factor. the amount of importance we give for future rewards when gamma is close to zero, agent favors immediate rewards. We want a HIGH gamma because the goal is to survive the whole lake.\n",
    "\n",
    "#### Equation\n",
    "Q[s, a] = Q[s, a] + alpha*(R + gamma*Max[Q(s’, A)] — Q[s, a])\n",
    "\n",
    "So at state 3 our equation might look like: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.6082\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .625\n",
    "y = .95\n",
    "num_episodes = 5000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "print (\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results\n",
    "\n",
    "### Changing number of episodes\n",
    ".95 gamma .8 alpha\n",
    "- 50: .0\n",
    "- 500: .164\n",
    "- 1000:.368\n",
    "- 5000:.5662\n",
    "- 10000: .6432\n",
    "- 15000: .4465\n",
    "### Changing Gamma\n",
    "\n",
    "\n",
    "\n",
    "reinforcement online learning\n",
    "Updates are meant to be very fast\n",
    "states in max...generally low\n",
    "very slow. play over \n",
    "time complexity very fast, but not guarneed \n",
    "if you're maximizing \n",
    "superistition \n",
    "    explortation vs exploitation: soft max vs a max\n",
    "    reward discount factor\n",
    "grid is space \n",
    "HOW CRITICAL IT IS THAT THE NUMBER OF STATES IS SO SMALL \n",
    "    slow way \n",
    "most of the work is doing eveything you can to minimize of the states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
